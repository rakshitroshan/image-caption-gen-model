{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical,plot_model\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,Embedding,Dropout,add\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=VGG16()\n",
    "model=Model(inputs=model.inputs,outputs=model.layers[-2].output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT FEATURES FROM IMAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d7563eba3b4eefb2c33dad19648bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features={}\n",
    "dir=os.path.join('D:/aiml/Images')\n",
    "for img in tqdm(os.listdir(dir)):\n",
    "    img_path=dir+'/'+img\n",
    "    image=load_img(img_path,target_size=(224,224))\n",
    "    image=img_to_array(image)\n",
    "    image=image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "    #preprocess od image\n",
    "    image=preprocess_input(image)\n",
    "    feature=model.predict(image,verbose=0)\n",
    "    img_id=img.split('.')[0]\n",
    "    features[img_id]=feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(feature,open(os.path.join('feature.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(working_dir,'feature.pkl'),'rb') as f:\n",
    "    features pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_txt = \"D:/aiml/captions.txt\"\n",
    "with open(os.path.join('D:/aiml/captions.txt'),'r') as f:\n",
    "    next(f)\n",
    "    captions_doc=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping={}\n",
    "for line in captions_doc.split('\\n'):\n",
    "    tokens=line.split('.')\n",
    "    img_id ,caption=tokens[0],tokens[1:]\n",
    "    img_id=img_id.split('.')[0]\n",
    "    caption=\" \".join(caption)\n",
    "    if img_id not in mapping:\n",
    "        mapping[img_id]=[]\n",
    "    mapping[img_id].append(caption) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8092\n"
     ]
    }
   ],
   "source": [
    "print(len(mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(mapping):\n",
    "    for key,captions in mapping.items():\n",
    "        for i in range(0,len(captions)):\n",
    "            caption=captions[i]\n",
    "            caption=caption.lower()\n",
    "            caption=caption.replace('jpg,','')\n",
    "            caption=caption.replace('start,','')\n",
    "            caption=caption.replace('[^A-Za-z]','')\n",
    "            caption=caption.replace('\\s',' ')\n",
    "            caption=\"startsen \"+\" \".join([word for word in caption.split() if len(word)>1])+\" endsen\"\n",
    "            captions[i]=caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startsen child in pink dress is climbing up set of stairs in an entry way endsen',\n",
       " 'startsen girl going into wooden building endsen',\n",
       " 'startsen little girl climbing into wooden playhouse endsen',\n",
       " 'startsen little girl climbing the stairs to her playhouse endsen',\n",
       " 'startsen little girl in pink dress going into wooden cabin endsen']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions=[]\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40456"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8477"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx_len=max(len(caption.split()) for caption in all_captions)\n",
    "mx_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id=list(mapping.keys())\n",
    "split=int(len(img_id)*0.90)\n",
    "train = img_id [:split]\n",
    "test = img_id [split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator to get data in batch (avoids session crash)\n",
    "def data_generator (data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "#loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions=mapping [key]\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "            #encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences ( [caption]) [0]\n",
    "                #split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    #split into input and output pairs\n",
    "                    in_seq, out_seq= seq[:i], seq[i]\n",
    "                    #pad input sequence\n",
    "                    in_seq = pad_sequences ([in_seq], maxlen=max_length) [0]\n",
    "                    # encode output sequence\n",
    "                    out_seq=to_categorical([out_seq], num_classes=vocab_size) [0]\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n==batch_size:\n",
    "                X1,X2,y=np.array(X1),np.array(X2),np.array(y)\n",
    "                yield[X1,X2],y\n",
    "                X1,X2,y=list(),list(),list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "inputs1=Input(shape=(4096,))\n",
    "fe1=Dropout(0.4)(inputs1)\n",
    "fe2=Dense(256,activation='relu')(fe1)\n",
    "inputs2=Input(shape=(mx_len,))\n",
    "se1=Embedding(vocab_size,256,mask_zero=True)(inputs2)\n",
    "se2=Dropout(0.4)(se1)\n",
    "se3=LSTM(256)(se2)\n",
    "\n",
    "decoder1=add([fe2,se3])\n",
    "decoder2=Dense(256,activation='relu')(decoder1)\n",
    "output=Dense(vocab_size,activation='softmax')(decoder2)\n",
    "\n",
    "model=Model(inputs=[inputs1,inputs2],outputs=output)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "plot_model(model,show_shapes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=15\n",
    "batch_size=64\n",
    "steps=len(train)//batch_size\n",
    "for i in range (epochs):\n",
    "#create data generator\n",
    "    generator = data_generator(train, mapping, features, tokenizer, mx_len, vocab_size, batch_size)\n",
    "#fit for one epoch\n",
    "    model.fit (generator, epochs=1, steps_per_epoch=steps, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer,tokenizer):\n",
    "    for word,index in tokenizer,word_index,items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image, tokenizer, max_length) :\n",
    "# add start tag for generation process\n",
    "    in_text = '<start>'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range (max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        #pad the sequence\n",
    "        sequence= pad_sequences([sequence], max_length) \n",
    "        # predict next word\n",
    "        yhat= model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat= np.argmax (yhat)\n",
    "        #convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "    #stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "    # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "    # stop if we reach end tag\n",
    "        if word =='<end>' :\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual,predicted=list(),list()\n",
    "for key in test:\n",
    "    captions=mapping[key]\n",
    "    y_pred=predict_caption(model,features[key],tokenizer,mx_len)\n",
    "    actual_caption=[caption.split() for caption in captions]\n",
    "    y_pred=y_pred.split()\n",
    "    actual.append(actual_caption)\n",
    "    predicted.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "def generate_caption(img_name):\n",
    "    image_name=\"\"\n",
    "    img_path=os.path.join(\"D:\\aiml\\Images\")\n",
    "    image=Image.open(img_path)\n",
    "    captions=mapping[img_id]\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    Y_pred=predict_caption(model,features[img_id],tokenizer,mx_len)\n",
    "    print(Y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
